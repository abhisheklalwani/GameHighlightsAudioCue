{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "mic = sr.Microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(recognizer, audio):\n",
    "        try:\n",
    "          print(recognizer.recognize_google(audio))\n",
    "        # handles any api/voice errors  errors\n",
    "        except sr.RequestError:\n",
    "          print( \"There was an issue in handling the request, please try again\")\n",
    "        except sr.UnknownValueError:\n",
    "          print(\"Unable to Recognize speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Abhishek\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Abhishek\\anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Abhishek\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\", line 690, in threaded_listen\n",
      "    with source as s:\n",
      "  File \"C:\\Users\\Abhishek\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\", line 134, in __enter__\n",
      "    assert self.stream is None, \"This audio source is already inside a context manager\"\n",
      "AssertionError: This audio source is already inside a context manager\n"
     ]
    }
   ],
   "source": [
    "with mic as source:\n",
    "    audio = r.listen_in_background(source, callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting speech recognition.\n",
      "Users message: 'hello'\n",
      "Users message: 'hello'\n",
      "Users message: 'what is up'\n",
      "Users message: 'that's cool'\n",
      "Users message: 'let's go'\n",
      "Users message: 'thank you'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7af5dd0b4107>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mphrase_time_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# listen to source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;31m# use testing api key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognize_google\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"en-US\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mlisten\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    650\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m                 \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHUNK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m  \u001b[1;31m# reached end of the stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m                 \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyaudio_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyaudio.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    606\u001b[0m                           paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_read_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting speech recognition.\")\n",
    "# https://github.com/Uberi/speech_recognition/blob/master/reference/library-reference.rst\n",
    "with sr.Microphone() as source:\n",
    "    while True:\n",
    "        try:\n",
    "            audio = r.listen(source,phrase_time_limit=2) # listen to source\n",
    "            # use testing api key\n",
    "            text = r.recognize_google(audio, language=\"en-US\")\n",
    "            print(\"Users message: '{}'\".format(text))\n",
    "            if text == 'capture':\n",
    "                print('saveHighlight')\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I could not understand you.\")\n",
    "        except sr.RequestError:\n",
    "            print(\"API call failed. Key valid? Internet connection?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from mss import mss\n",
    "from PIL import ImageGrab\n",
    "\n",
    "\n",
    "sct = mss()\n",
    "img_np = []\n",
    "\n",
    "out = cv2.VideoWriter('output.mp4', -1, 20.0, (640,480))\n",
    "\n",
    "while 1:\n",
    "    img = ImageGrab.grab(bbox=(0,0,960,540)) #bbox specifies specific region (bbox= x,y,width,height *starts top-left)\n",
    "    img_np = np.array(img) #this is the array obtained from conversion\n",
    "    frame = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "    # cv2.imshow(\"test\", frame)\n",
    "    out.write(frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "\n",
    "# display screen resolution, get it from your OS settings\n",
    "SCREEN_SIZE = (1920, 1080)\n",
    "# define the codec\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "# create the video write object\n",
    "out = cv2.VideoWriter(\"output.avi\", fourcc, 20.0, (1920,1080))\n",
    "\n",
    "while True:\n",
    "    # make a screenshot\n",
    "    img = pyautogui.screenshot()\n",
    "    # convert these pixels to a proper numpy array to work with OpenCV\n",
    "    frame = np.array(img)\n",
    "    # convert colors from BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # write the frame\n",
    "    out.write(frame)\n",
    "    # show the frame\n",
    "    cv2.imshow(\"screenshot\", frame)\n",
    "    # if the user clicks q, it exits\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# make sure everything is closed when exited\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture(0)\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "out = cv.VideoWriter('output.avi', fourcc, 20.0, (640,  480))\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    frame = cv.flip(frame, 0)\n",
    "    # write the flipped frame\n",
    "    out.write(frame)\n",
    "    cv.imshow('frame', frame)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mss import mss\n",
    "\n",
    "\n",
    "def record(name):\n",
    "    with mss() as sct:\n",
    "        # mon = {'top': 160, 'left': 160, 'width': 200, 'height': 200}\n",
    "        mon = sct.monitors[0]\n",
    "        name = name + '.mp4'\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        desired_fps = 30.0\n",
    "        out = cv2.VideoWriter(name, fourcc, desired_fps,\n",
    "                              (mon['width'], mon['height']))\n",
    "        last_time = 0\n",
    "        while True:\n",
    "            img = sct.grab(mon)\n",
    "            # cv2.imshow('test', np.array(img))\n",
    "            if time.time() - last_time > 1./desired_fps:\n",
    "                last_time = time.time()\n",
    "                destRGB = cv2.cvtColor(np.array(img), cv2.COLOR_BGRA2BGR)\n",
    "                out.write(destRGB)\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "\n",
    "record(\"Video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting speech recognition.Listening!\n",
      "\n",
      "Sorry, I could not understand you.\n",
      "Sorry, I could not understand you.\n",
      "Sorry, I could not understand you.\n",
      "Users message: 'hello'\n",
      "Users message: 'what are you doing'\n",
      "Users message: 'can you recognize me'\n",
      "Users message: 'speed time'\n",
      "Users message: 'so you do Miss words in between'\n",
      "Users message: 'why is the m capital'\n",
      "Users message: 'why are pretty accurate'\n",
      "Users message: 'nevermind'\n",
      "Users message: 'capture'\n",
      "Saving Highlight as requested\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "\n",
    "\n",
    "r = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "\n",
    "# display screen resolution, get it from your OS settings\n",
    "SCREEN_SIZE = (1920, 1080)\n",
    "# define the codec\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "# create the video write object\n",
    "out = cv2.VideoWriter(\"highlight.avi\", fourcc, 10.0, (1920,1080))\n",
    "\n",
    "buffer = []\n",
    "\n",
    "breakthread1 = 0\n",
    "\n",
    "def maintain_buffer():\n",
    "    while True:\n",
    "        # make a screenshot\n",
    "        img = pyautogui.screenshot()\n",
    "        # convert these pixels to a proper numpy array to work with OpenCV\n",
    "        frame = np.array(img)\n",
    "        # convert colors from BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # write the frame\n",
    "        if len(buffer) < 100:\n",
    "            buffer.append(frame)\n",
    "        else:\n",
    "            buffer.pop(0)\n",
    "            buffer.append(frame)\n",
    "        if breakthread1 == 1:\n",
    "            break\n",
    "\n",
    "def capture_audio():\n",
    "    print(\"Starting speech recognition.\")\n",
    "    # https://github.com/Uberi/speech_recognition/blob/master/reference/library-reference.rst\n",
    "    with sr.Microphone() as source:\n",
    "        while True:\n",
    "            try:\n",
    "                audio = r.listen(source,phrase_time_limit=2) # listen to source\n",
    "                # use testing api key\n",
    "                text = r.recognize_google(audio, language=\"en-US\")\n",
    "                print(\"Users message: '{}'\".format(text))\n",
    "                if text == 'capture':\n",
    "                    print(\"Saving Highlight as requested\")\n",
    "                    breakthread1 = 1\n",
    "                    balloon_tip('Highlight saved','Your highlight is being saved at C:/Users/abhishekl')\n",
    "                    for i in range(len(buffer)):\n",
    "                        out.write(buffer[i])\n",
    "                    out.release()\n",
    "                    break\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Sorry, I could not understand you.\")\n",
    "            except sr.RequestError:\n",
    "                print(\"API call failed. Key valid? Internet connection?\")\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    # creating thread\n",
    "    t1 = threading.Thread(target=maintain_buffer) \n",
    "    t2 = threading.Thread(target=capture_audio) \n",
    "  \n",
    "    # starting thread 1\n",
    "    t1.start()\n",
    "    # starting thread 2\n",
    "    t2.start()\n",
    "  \n",
    "    # both threads completely executed \n",
    "    print(\"Listening!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notify2\n",
    "ICON_PATH = \"C:/Users/abhishekl/Desktop/Latest/GFN_logo1024.png\"\n",
    "notify2.init(\"Highlight Notifier\")\n",
    "n = notify2.Notification(None, icon = ICON_PATH)\n",
    "n.set_urgency(notify2.URGENCY_CRITICAL)\n",
    "n.set_timeout(5000)\n",
    "n.update(\"Your highlight is saved successfully\")\n",
    "n.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- coding: utf-8 --\n",
    " \n",
    "from win32api import *\n",
    "from win32gui import *\n",
    "import win32con\n",
    "import sys, os\n",
    "import struct\n",
    "import time\n",
    "\n",
    "class WindowsBalloonTip:\n",
    "    def __init__(self, title, msg):\n",
    "        message_map = {\n",
    "                win32con.WM_DESTROY: self.OnDestroy,\n",
    "        }\n",
    "        # Register the Window class.\n",
    "        wc = WNDCLASS()\n",
    "        hinst = wc.hInstance = GetModuleHandle(None)\n",
    "        wc.lpszClassName = \"PythonTaskbar\"\n",
    "        wc.lpfnWndProc = message_map # could also specify a wndproc.\n",
    "        classAtom = RegisterClass(wc)\n",
    "        # Create the Window.\n",
    "        style = win32con.WS_OVERLAPPED | win32con.WS_SYSMENU\n",
    "        self.hwnd = CreateWindow( classAtom, \"Taskbar\", style, \\\n",
    "                0, 0, win32con.CW_USEDEFAULT, win32con.CW_USEDEFAULT, \\\n",
    "                0, 0, hinst, None)\n",
    "        UpdateWindow(self.hwnd)\n",
    "        iconPathName = os.path.abspath(os.path.join( sys.path[0], \"balloontip.ico\" ))\n",
    "        icon_flags = win32con.LR_LOADFROMFILE | win32con.LR_DEFAULTSIZE\n",
    "        try:\n",
    "           hicon = LoadImage(hinst, iconPathName, \\\n",
    "                    win32con.IMAGE_ICON, 0, 0, icon_flags)\n",
    "        except:\n",
    "          hicon = LoadIcon(0, win32con.IDI_APPLICATION)\n",
    "        flags = NIF_ICON | NIF_MESSAGE | NIF_TIP\n",
    "        nid = (self.hwnd, 0, flags, win32con.WM_USER+20, hicon, \"tooltip\")\n",
    "        Shell_NotifyIcon(NIM_ADD, nid)\n",
    "        Shell_NotifyIcon(NIM_MODIFY, \\\n",
    "                         (self.hwnd, 0, NIF_INFO, win32con.WM_USER+20,\\\n",
    "                          hicon, \"Balloon  tooltip\",title,200,msg))\n",
    "        # self.show_balloon(title, msg)\n",
    "        time.sleep(10)\n",
    "        DestroyWindow(self.hwnd)\n",
    "    def OnDestroy(self, hwnd, msg, wparam, lparam):\n",
    "        nid = (self.hwnd, 0)\n",
    "        Shell_NotifyIcon(NIM_DELETE, nid)\n",
    "        PostQuitMessage(0) # Terminate the app.\n",
    "def balloon_tip(title, msg):\n",
    "    w=WindowsBalloonTip(msg, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
